{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Optimal Classifier Compared With a Simple Neural Network\n",
    "\n",
    "In this notebook, we will explore the ability to differentiate between classes of data using either a simple \"optimal classifier\" based on a log-likelihood, or a neural network.\n",
    "\n",
    "In your homework, you're going to focus on a simpler task, but you're going to build your neural network from the ground up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started and import some packages that we'll need throughout the tutorial. Make sure that you've installed:\n",
    "\n",
    "* `sklearn`\n",
    "* `h5py`\n",
    "* `keras`\n",
    "* `tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Optimal Classifier\n",
    "\n",
    "We will start by implementing the optimal (Gaussian) 1D classifier, as discussed in the lecture notes.\n",
    "\n",
    "Numpy provides the function [numpy.random.normal](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.normal.html), which is very useful for sampling a Gaussian with a mean 'mu' and standard deviation 'sigma'. You can try out this function yourself in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of building a Gaussian PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 samples from a Gaussian pdf with mu=5 and sigma=2\n",
    "mu = 5\n",
    "sigma = 2\n",
    "number_of_samples = 1000\n",
    "test_samples = np.random.normal(mu, sigma, number_of_samples)\n",
    "\n",
    "# Plotting with matplotlib\n",
    "\n",
    "# First clear the figures\n",
    "plt.clf()\n",
    "# Segment the canvas into upper and lower subplots\n",
    "plt.subplot(211)\n",
    "# Plot the random numbers\n",
    "plt.plot(test_samples)\n",
    "plt.subplot(212)\n",
    "# Histogram the numbers\n",
    "plt.hist(test_samples, bins=100)\n",
    "# Display the canvas\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Gaussian PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some data. Below we have provided you with a function that gives you two cases:\n",
    "1. ```'do_superposition = False'```: Two Gaussians, both with a standard deviation of 1, but one with mean +1 and the other with mean -1. \n",
    "2. ```'do_superposition = True'```: Signal and background are the superposition of two Gaussians whose parameters are given below. By construction, the overlap between signal and background is larger in this second case.\n",
    "\n",
    "You are encouraged to experiment with your own function after you have run through the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that generates N signal and N background samples (note that 'do_simple' is true by default)\n",
    "def generate_samples(N, do_superposition=False):\n",
    "    \n",
    "    # Case 1: Signal and background are each a single Gaussian\n",
    "    if not do_superposition:\n",
    "        \n",
    "        # Signal Gaussian has mean 1 and standard deviation 1\n",
    "        mu1, sigma1 = 1, 1\n",
    "        signal = np.random.normal(mu1, sigma1, N)\n",
    "        \n",
    "        # Background Gaussian has mean 1 and standard deviation 1\n",
    "        mu1, sigma1 = -1, 1\n",
    "        background = np.random.normal(mu1, sigma1, N)\n",
    "        \n",
    "    # Case 2: Signal and background are superpositions of Gaussians\n",
    "    else:\n",
    "        \n",
    "        mu1a, sigma1a = -1.1, 0.5\n",
    "        x1a = np.random.normal(mu1a, sigma1a, int(0.6*N))\n",
    "        mu1b, sigma1b = 1, 1\n",
    "        x1b = np.random.normal(mu1b, sigma1b, int(0.4*N))\n",
    "        \n",
    "        mu2a, sigma2a = 2, 0.5\n",
    "        x2a = np.random.normal(mu2a, sigma2a, int(0.7*N))\n",
    "        mu2b, sigma2b = -1, 1\n",
    "        x2b = np.random.normal(mu2b, sigma2b, int(0.3*N))\n",
    "\n",
    "        signal = np.append(x1a,x1b)\n",
    "        background = np.append(x2a,x2b)\n",
    "    \n",
    "\n",
    "    return signal, background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's actually run the function and generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If 'do_superposition = True' we get multiple Gaussians\n",
    "do_superposition = False\n",
    "# Number of samples\n",
    "N = 10000000\n",
    "# Number of bins in the histograms\n",
    "nbins = 500\n",
    "\n",
    "# Generate signal and background\n",
    "signal, background = generate_samples(N, do_superposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a dataset with h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often when dealing with data you may want to either save something you generated or load data you got from somebody else. One of the most common formats is HDF5, a \"data model, library, and file format for storing and managing data.\" It is also the most common storage format in data science. \n",
    "\n",
    "`h5py` provides a python API for HDF5. In most cases, you do not need to know very much about HDF5 or `h5py`, just how to read/write tensors into/from files, which you can easily pick up from the h5py (Quick Start)[http://docs.h5py.org/en/latest/quick.html]. \n",
    "\n",
    "Even though this tutorial could be completed without storing and retreiving the data, it may be useful to go through that exercise. \n",
    "\n",
    "Let's start by save the data you generated to HDF5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that you have h5py installed!\n",
    "import h5py\n",
    "\n",
    "# create a new file\n",
    "h5_file = h5py.File(\"data1.h5\", \"w\")\n",
    "h5_file.create_dataset('signal', data=signal)\n",
    "h5_file.create_dataset('background', data=background)\n",
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can retreive the data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file_readonly = h5py.File('data1.h5','r')\n",
    "signal = h5_file_readonly['signal'][:]\n",
    "background = h5_file_readonly['background'][:]\n",
    "h5_file_readonly.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create histograms and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histograms\n",
    "plt.clf()\n",
    "plt.hist(signal, 50, density=1, facecolor='blue', alpha=0.75, label='S')\n",
    "plt.hist(background, 50, density=1, facecolor='red', alpha=0.75, label='B')\n",
    "plt.xlabel('Input feature x')\n",
    "plt.ylabel('Probability density (arbitrary units)')\n",
    "plt.title(r'Signal and Background')\n",
    "plt.legend(loc='upper right')\n",
    "plt.axis([-5, 5, 0, 0.7])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The log-likelihood ratio (LLR)\n",
    "\n",
    "Next, let's compute the log-likelihood ratio (LLR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histograms, which we will use for calculating the log-likelihood ratio (LLR) below\n",
    "h_signal     = np.histogram(signal, bins=500, range=(-5,5))\n",
    "h_background = np.histogram(background, bins=500, range=(-5,5))\n",
    "\n",
    "LL_dict = {} # used only for plotting\n",
    "LL_dict_bybin = {} # used for computing\n",
    "\n",
    "for i in range(len(h_signal[0])):\n",
    "    \n",
    "    # the if statements are there to account for \"binning effects\"\n",
    "    if (h_background[0][i] > 0 and h_signal[0][i] > 0):\n",
    "        LL_dict[h_background[1][i]] = np.log(1.*h_signal[0][i]/h_background[0][i])\n",
    "    elif (h_signal[0][i] > 0): # in case background bin = 0\n",
    "        LL_dict[h_background[1][i]] = np.log(100000.) #huge number\n",
    "    elif (h_background[0][i] > 0): # in case signal bin = 0\n",
    "        LL_dict[h_background[1][i]] = np.log(1./100000.) #very small number\n",
    "    else: \n",
    "        LL_dict[h_background[1][i]] = np.log(1.)\n",
    "    \n",
    "    LL_dict_bybin[i] = LL_dict[h_background[1][i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the mapping between the input feature vector and the LLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of 'x' values\n",
    "xvals = [d for d in LL_dict]\n",
    "# array of 'y' values\n",
    "yvals = [LL_dict[d] for d in LL_dict]\n",
    "\n",
    "xvals = np.array(xvals)\n",
    "yvals = np.array(yvals)\n",
    "\n",
    "# Return the indices that result from sorting the array (but do not modify the array itself)\n",
    "index_sorted = xvals.argsort()\n",
    "# Sort the arrays \n",
    "xvals = xvals[index_sorted[::-1]]\n",
    "yvals = yvals[index_sorted[::-1]]\n",
    "\n",
    "# Plot the LLR as a function of input feature x\n",
    "plt.clf()\n",
    "plt.plot(xvals,yvals)\n",
    "plt.xlabel('Input feature x')\n",
    "plt.ylabel('Log Likelihood Ratio')\n",
    "plt.title(r'LLR as a function of x')\n",
    "plt.axis([-6,6,-6,6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about the 'bumps' towards the edges of the plot, that's just due to low statistics.\n",
    "\n",
    "The next step is to compute and plot the PDF of the LLR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of bins in the histgrams\n",
    "nbins = 50\n",
    "\n",
    "# Create histograms\n",
    "h_signal_yvals = np.histogram([], bins=nbins, range=(-10,10))\n",
    "h_background_yvals = np.histogram([], bins=nbins, range=(-10,10))\n",
    "\n",
    "# Fill histograms\n",
    "for i in range(len(h_signal[0])):\n",
    "    whichbin = np.digitize(LL_dict[h_signal[1][i]], h_signal_yvals[1])\n",
    "    if (whichbin > 49):\n",
    "        whichbin = 49\n",
    "    h_signal_yvals[0][whichbin]+=h_signal[0][i]\n",
    "    h_background_yvals[0][whichbin]+=h_background[0][i]\n",
    "\n",
    "# Plot the PDF of the LLR\n",
    "plt.clf()\n",
    "plt.xlabel('Log Likelihood Ratio')\n",
    "plt.ylabel('Probability Density (arbitrary units)')\n",
    "plt.title(r'Signal and Background')\n",
    "plt.bar(h_signal_yvals[1][:-1],h_signal_yvals[0], width=h_signal_yvals[1][1]-h_signal_yvals[1][0],facecolor='blue', alpha=0.75, label='S')\n",
    "plt.bar(h_background_yvals[1][:-1],h_background_yvals[0], width=h_background_yvals[1][1]-h_background_yvals[1][0],facecolor='red', alpha=0.75, label='B')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC) curve\n",
    "\n",
    "See also: [Receiver operating characteristic (Wikipedia)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "\n",
    "Finally, we can scan a threshold cut on the LLR and make the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the ROC curve\n",
    "ROCx = np.zeros(nbins)\n",
    "ROCy = np.zeros(nbins)\n",
    "intx = 0.\n",
    "inty = 0.\n",
    "\n",
    "for i in range(nbins):\n",
    "    intx+=h_signal_yvals[0][i]\n",
    "    inty+=h_background_yvals[0][i]\n",
    "\n",
    "for i in range(nbins):\n",
    "    sum_signal = 0.\n",
    "    sum_background = 0.\n",
    "    for j in range(i,len(h_signal_yvals[1])-1):\n",
    "        sum_signal+=h_signal_yvals[0][j]\n",
    "        sum_background+=h_background_yvals[0][j]\n",
    "\n",
    "    ROCx[i] = sum_signal/intx\n",
    "    ROCy[i] = sum_background/inty\n",
    "    \n",
    "# Plot the ROC curve\n",
    "plt.clf()\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.plot(ROCx,ROCy,label=\"LLR\")\n",
    "plt.plot([0,1],[0,1],linestyle='--',color=\"#C0C0C0\",label=\"Random\")\n",
    "plt.xlabel('Pr(label signal | signal)')\n",
    "plt.ylabel('Pr(label signal | background)')\n",
    "plt.title(r'ROC Curve')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the first part of this tutorial. Now let's do some Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement and train a simple Neural Network in Keras\n",
    "\n",
    "Before one gets to the actual training, it's often necessary to transform the data into numpy array of the correct shape. To better understand what's going on below, let's do a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Say we have two 1x3 arrays (e.g. A=signal, B=background)\n",
    "A = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32)\n",
    "B = np.array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20], dtype=np.float32)\n",
    "\n",
    "# We want to have labels '1' associated with the signal (A) and labels '0' associated with the background (B)\n",
    "A_labels = np.ones(10)\n",
    "B_labels = np.zeros(10)\n",
    "\n",
    "print('\\nA: {}'.format(A))\n",
    "print('B: {}'.format(B))\n",
    "print('\\nA_labels: {}'.format(A_labels))\n",
    "print('B_labels: {}\\n'.format(B_labels))\n",
    "\n",
    "# We can concatenate the A and B arrays, and the A_labels and B_labels array like this\n",
    "C = np.concatenate((A,B))\n",
    "C_labels = np.concatenate((A_labels,B_labels))\n",
    "\n",
    "print('\\nC: {}'.format(C))\n",
    "print('C_labels: {}'.format(C_labels))\n",
    "\n",
    "# Before training on the a dataset one often want to split it up into a 'training set' and a 'test set'\n",
    "# There is a useful function in scikit-learn that does this for you\n",
    "# This function also scrambles the examples \n",
    "from sklearn.model_selection import train_test_split\n",
    "C_train, C_test, C_labels_train, C_labels_test, = train_test_split(C, C_labels, test_size=3, random_state=1)\n",
    "\n",
    "# If this seems confusing, taking a look at the print output below should hopefully make things clear\n",
    "print('\\nC_train: {}'.format(C_train))\n",
    "print('C_labels_train: {}'.format(C_labels_train))\n",
    "\n",
    "print('\\nC_test: {}'.format(C_test))\n",
    "print('\\nC_labels_test: {}'.format(C_labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another things is that Keras require the 'X' inputs to be formatted in a certain way. Here's a simple example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "print(A)\n",
    "AT = np.array(A)[np.newaxis].T\n",
    "print(AT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we hopefully understand a little more about how to manipulate numpy arrays, let's prepare our actual data for training a Neural Network in Keras.\n",
    "\n",
    "If you already executed the cells for generating (or loading) data in the first part of the tutorial, you should be good to go, otherwise please scroll up and execute the cell that calls:\n",
    "\n",
    "```signal, background = generate_samples(N)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of signal + background events\n",
    "n_signal = len(signal)\n",
    "n_background = len(background)\n",
    "n_total = len(signal) + len(background)\n",
    "\n",
    "# use 90% of the total number of events for training the network\n",
    "n_train = int(0.9*n_total)\n",
    "# use the remaning 10% for testing\n",
    "n_test = n_total-n_train\n",
    "\n",
    "# generate an array of ones as signal labels\n",
    "sig_labels = np.ones(n_signal)\n",
    "\n",
    "# generate an array of zeros as background labels\n",
    "bkg_labels = np.zeros(n_background)\n",
    "\n",
    "# concatenate the signal and background samples\n",
    "X = np.concatenate((signal,background))\n",
    "y = np.concatenate((sig_labels,bkg_labels))\n",
    "\n",
    "# Format the inputs for Keras\n",
    "X = np.array(X)[np.newaxis].T\n",
    "\n",
    "# split the dataset into a training and a validation set and scamble the inputs (as illustrated above)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, = train_test_split(X, y, test_size=n_test, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we get to the fun stuff, let's implement our first Neural Network in Keras. To learn more what all this does, you are strongly encouraged to go and read the [Keras documentation](https://keras.io).\n",
    "\n",
    "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some layers to our model with \"Dense\". Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
    "\n",
    "In other words, Dense is just your the regular densely-connected NN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# Since our samples are only the X values (of either signal or background), the first layer just has one input dimension\n",
    "model.add(Dense(1, input_dim=1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# We then implement only one hidden layer with 8 neurons (you can experiment with changing this number)\n",
    "n_neurons_hidden = 8\n",
    "model.add(Dense(n_neurons_hidden, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# Finally we add one output layer\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compile our model and choose ['binary_crossentropy']('https://keras.io/losses/') as our loss function and ['ADAM' (adaptive moment estimation)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) as the optimizer (an extension of stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train our model in the training data. Keras automatically runs validation on the test data. \n",
    "\n",
    "You can experiment with the number of 'epochs' the 'batch size': \n",
    "- one epoch = One forward pass and one backward pass of all the training examples.\n",
    "- batch size = The number of training examples in one forward/backward pass. Tthe higher the batch size, the more memory space you'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=3, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This very simple model should converge very quickly (even after 1-3 epochs). Training more complicated networks can take a very long time (days, weeks, or even months). \n",
    "\n",
    "The model history keeps track of the loss and accuracy for each epoch. Note that the training above was setup to run on the validation sample at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the loss versus epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history=history.history[\"loss\"]\n",
    "plt.plot(range(len(loss_history)),loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the ROC curve and compare it to the result we got form the 1D optimal classifier above.\n",
    "\n",
    "The quantities returned here are:\n",
    "\n",
    "* sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "$$\n",
    "\\mathrm {TPR} ={\\frac {\\mathrm {TP} }{P}}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }}=1-\\mathrm {FNR}\n",
    "$$\n",
    "\n",
    "* fall-out or false positive rate (FPR)\n",
    "$$\n",
    "\\mathrm {FPR} ={\\frac {\\mathrm {FP} }{N}}={\\frac {\\mathrm {FP} }{\\mathrm {FP} +\\mathrm {TN} }}=1-\\mathrm {TNR}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make use of a function in scikit-learn to calculate the ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_test, model.predict(X_test))\n",
    "                        \n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve from the NN and overlay the ROC curve from the 1D classifier\n",
    "plt.clf()\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.plot(ROCx,ROCy,label=\"LLR\")\n",
    "plt.plot(tpr,fpr,color='darkorange',label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0,1],[0,1],linestyle='--',color=\"#C0C0C0\",label=\"Random\")\n",
    "plt.xlabel('Pr(label signal | signal)')\n",
    "plt.ylabel('Pr(label signal | background)')\n",
    "plt.title(r'ROC Curve')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: try changing things\n",
    "\n",
    "Tt's time to start tweaking some parameters and play around with the code. Here are a few suggestions of what we will try:\n",
    "\n",
    "#### Try changing the number of samples (N)\n",
    "What happens if you reduce the number of samples by an order of magnitude or more? If you don't see any effect, try to reduce it further. Which model does better as the number of events goes to zero? Why?\n",
    "\n",
    "#### Vary the Network structure\n",
    "What happens if you increase the number neurons in the hidden layer? What happens if you increase/decrease the number of training epochs? What happens if you vary the batch size? What happens if you change the [activation function](https://keras.io/activations/) (in particular try switching from 'sigmoind' to 'linear')?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done everything correctly, you should find that the ROC curves should overlap nicely.\n",
    "\n",
    "Congratulations, you are now done with this tutorial. Head over to [keras.io](http://keras.io) and find some fun tutorials!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
